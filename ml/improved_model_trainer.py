# ml/improved_model_trainer.py - –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è kNN –º–æ–¥–µ–ª–∏

import numpy as np
from typing import Tuple, Dict, List, Optional
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import StandardScaler
import pickle
import os
from datetime import datetime

from ml.feature_extractor import FeatureExtractor
from config import MODELS_DIR, MIN_TRAINING_SAMPLES

class ImprovedModelTrainer:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è kNN –º–æ–¥–µ–ª–∏ –¥–ª—è –±–∏–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.feature_extractor = FeatureExtractor()
        self.scaler = StandardScaler()
        self.model = None
        self.calibrated_model = None
        self.best_params = {}
        self.training_stats = {}
        
    def prepare_training_data(self, positive_samples: List) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        X_positive = self.feature_extractor.extract_features_from_samples(positive_samples)
        n_positive = len(X_positive)
        
        if n_positive < MIN_TRAINING_SAMPLES:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {n_positive}, –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {MIN_TRAINING_SAMPLES}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        X_negative = self._generate_realistic_negatives(X_positive, factor=1.0)
        n_negative = len(X_negative)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(n_positive), np.zeros(n_negative)])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_normalized = self.scaler.fit_transform(X)
        
        print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {n_positive} –ª–µ–≥–∏—Ç–∏–º–Ω—ã—Ö, {n_negative} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö")
        return X_normalized, y
    
    def _generate_realistic_negatives(self, X_positive: np.ndarray, factor: float = 1.0) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–æ–ª–µ–µ —Ä–∞–∑–ª–∏—á–∏–º—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        n_samples = len(X_positive)
        n_negatives = int(n_samples * factor)
    
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
    
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –û–ë–£–ß–ê–Æ–©–ò–• –î–ê–ù–ù–´–•:")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—à–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_samples}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {mean}")
        print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std}")
    
        negatives = []
    
        for i in range(n_negatives):
            # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü –∫–∞–∫ –æ—Å–Ω–æ–≤—É
            base_sample = X_positive[np.random.randint(0, len(X_positive))].copy()
        
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –∏–∑–º–µ–Ω—è–µ–º –Ω–∞ 2-4 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            change_magnitude = np.random.uniform(2.0, 4.0)  # 2-4 —Å–∏–≥–º—ã
            direction = np.random.choice([-1, 1], size=len(base_sample))
        
            # –ò–∑–º–µ–Ω—è–µ–º 2-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ
            features_to_change = np.random.choice(len(base_sample), 
                                            size=np.random.randint(2, 4), 
                                            replace=False)
        
            modified_sample = base_sample.copy()
            for idx in features_to_change:
                if std[idx] > 0:
                    change = direction[idx] * change_magnitude * std[idx]
                    modified_sample[idx] = base_sample[idx] + change
        
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å
            modified_sample = np.maximum(modified_sample, mean * 0.2)
            modified_sample = np.minimum(modified_sample, mean * 5.0)
        
            negatives.append(modified_sample)
    
        negatives_array = np.array(negatives)
    
        # –û—Ç–ª–∞–¥–∫–∞
        print(f"–°—Ä–µ–¥–Ω–µ–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {np.mean(negatives_array, axis=0)}")
    
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(X_positive, negatives_array)
        min_dist = np.min(distances)
        mean_dist = np.mean(distances)
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {min_dist:.3f}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {mean_dist:.3f}")
    
        return negatives_array
    
    def train_user_model(self, positive_samples: List) -> Tuple[bool, float, str]:
        """–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            print(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º training_stats
            self.training_stats = {}
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self.prepare_training_data(positive_samples)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.25, random_state=42, stratify=y
            )
            
            # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_params = self._optimize_hyperparameters(X_train, y_train)
            
            # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.model = KNeighborsClassifier(**best_params)
            self.model.fit(X_train, y_train)
            
            # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            self.calibrated_model = CalibratedClassifierCV(
                self.model, method='isotonic', cv=3
            )
            self.calibrated_model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            test_accuracy = self._evaluate_model(X_test, y_test)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è (–æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ _evaluate_model)
            self.training_stats.update({
                'user_id': self.user_id,
                'training_samples': len(positive_samples),
                'total_samples': len(X),
                'best_params': best_params,
                'test_accuracy': test_accuracy,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                'training_date': datetime.now().isoformat()
            })
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self._save_model()
            
            print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.2%}")
            return True, test_accuracy, f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {test_accuracy:.2%}"
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False, 0.0, f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}"
    
    def _optimize_hyperparameters(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Grid Search"""
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        param_grid = {
            'n_neighbors': range(5, min(20, len(X_train) // 3)),  # –ë–æ–ª—å—à–µ —Å–æ—Å–µ–¥–µ–π
            'weights': ['uniform'],  # –¢–æ–ª—å–∫–æ uniform –≤–µ—Å–∞
            'metric': ['euclidean', 'manhattan'],
            'algorithm': ['auto', 'ball_tree']
        }
        
        # Grid Search —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        grid_search = GridSearchCV(
            KNeighborsClassifier(),
            param_grid,
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            return_train_score=False
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        print(f"CV —Ç–æ—á–Ω–æ—Å—Ç—å: {grid_search.best_score_:.3f}")
        
        self.best_params = grid_search.best_params_
        return grid_search.best_params_
    
    def _evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        print(f"–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:")
        print(f"  Accuracy: {accuracy:.3f}")
        print(f"  Precision: {precision:.3f}")
        print(f"  Recall: {recall:.3f}")
        print(f"  F1-score: {f1:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ROC-–∫—Ä–∏–≤–æ–π
        self.training_stats.update({
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'y_test': y_test.tolist(),  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            'y_proba': y_proba.tolist()  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        })
        
        return accuracy
    
    def predict(self, features: np.ndarray) -> Tuple[bool, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ë–ï–ó –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (–≤—Ä–µ–º–µ–Ω–Ω–æ)"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
    
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_scaled = self.scaler.transform(features.reshape(1, -1))
    
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û —Å—ã—Ä—É—é –º–æ–¥–µ–ª—å
        raw_proba = self.model.predict_proba(features_scaled)[0]
    
        print(f"üîç –û–¢–õ–ê–î–ö–ê –ë–ï–ó –ö–ê–õ–ò–ë–†–û–í–ö–ò:")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–∏: {features}")
        print(f"   –°—ã—Ä–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {raw_proba}")
    
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        confidence = raw_proba[1] if len(raw_proba) > 1 else raw_proba[0]
    
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.5 –¥–ª—è —Å—ã—Ä–æ–π –º–æ–¥–µ–ª–∏
        is_legitimate = confidence >= 0.1
    
        return is_legitimate, confidence
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{self.user_id}_improved_knn.pkl")
        
        model_data = {
            'model': self.model,
            'calibrated_model': self.calibrated_model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    @classmethod
    def load_model(cls, user_id: int) -> Optional['ImprovedModelTrainer']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_improved_knn.pkl")
        
        if not os.path.exists(model_path):
            return None
        
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            trainer = cls(user_id)
            trainer.model = model_data['model']
            trainer.calibrated_model = model_data['calibrated_model']
            trainer.scaler = model_data['scaler']
            trainer.best_params = model_data['best_params']
            trainer.training_stats = model_data.get('training_stats', {})
            
            return trainer
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def get_model_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            'is_trained': self.model is not None,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }