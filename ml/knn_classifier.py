# ml/knn_classifier.py - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é

import numpy as np
from typing import Tuple, Optional, List
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import pickle

from config import KNN_NEIGHBORS, MODELS_DIR
import os

class KNNAuthenticator:
    """KNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –¥–∏–Ω–∞–º–∏–∫–µ –Ω–∞–∂–∞—Ç–∏–π"""
    

    def __init__(self, n_neighbors: int = KNN_NEIGHBORS):
        self.n_neighbors = min(n_neighbors, 5)  # –£–≤–µ–ª–∏—á–∏–º –¥–æ 5 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        self.model = KNeighborsClassifier(
            n_neighbors=self.n_neighbors,
            metric='euclidean',
            weights='distance',     # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –≤–∞–∂–Ω–µ–µ
            algorithm='ball_tree'   # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        )
        self.is_trained = False
        self.normalization_stats = None
        self.training_data = None
        
    def train(self, X_positive: np.ndarray, X_negative: np.ndarray = None) -> Tuple[bool, float]:
        """–û–±—É—á–µ–Ω–∏–µ —Å –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
        n_samples = len(X_positive)
        if n_samples < 5:
            return False, 0.0
    
        print(f"\nüéØ –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï")
        print(f"–¢–≤–æ–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_samples}")
    
        self.training_data = X_positive.copy()
    
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if X_negative is None or len(X_negative) == 0:
            X_negative = self._generate_balanced_negatives(X_positive)
    
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 2:1 –≤ –ø–æ–ª—å–∑—É —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–±—ã–ª–æ 3:1 –∏–ª–∏ 4:1)
        neg_count = max(n_samples // 2, 10)  # –ú–∏–Ω–∏–º—É–º 10, –æ–±—ã—á–Ω–æ –≤ 2 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
    
        if len(X_negative) > neg_count:
            # –ë–µ—Ä–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–Ω–µ —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –¥–∞–ª–µ–∫–∏–µ)
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances(X_negative, X_positive)
            min_distances = np.min(distances, axis=1)
            
            # –ë–µ—Ä–µ–º 50% –¥–∞–ª–µ–∫–∏—Ö + 50% —Å—Ä–µ–¥–Ω–∏—Ö (–Ω–µ —Ç–æ–ª—å–∫–æ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ)
            far_count = neg_count // 2
            medium_count = neg_count - far_count
            
            # –î–∞–ª–µ–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã
            far_indices = np.argsort(min_distances)[-far_count:]
            
            # –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–∏–º–µ—Ä—ã (–∏—Å–∫–ª—é—á–∞—è —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–ª–µ–∫–∏–µ)
            remaining_indices = np.setdiff1d(np.arange(len(X_negative)), far_indices)
            remaining_distances = min_distances[remaining_indices]
            medium_indices = remaining_indices[np.argsort(remaining_distances)[len(remaining_distances)//4:len(remaining_distances)//4+medium_count]]
            
            selected_indices = np.concatenate([far_indices, medium_indices])
            X_negative = X_negative[selected_indices]
    
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(X_positive)} –¢–í–û–ò–• vs {len(X_negative)} –ß–£–ñ–ò–•")
        print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(X_positive)/(len(X_positive)+len(X_negative))*100:.0f}% —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    
        # –û–±—É—á–µ–Ω–∏–µ
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(len(X_positive)), np.zeros(len(X_negative))])
    
        self.model.fit(X, y)
        self.is_trained = True
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        train_accuracy = self.model.score(X, y)
        print(f"Train accuracy: {train_accuracy:.3f}")

        self.test_data = {
            'X_positive': X_positive.copy(),
            'X_negative': X_negative.copy() if X_negative is not None else None,
            'y_positive': np.ones(len(X_positive)),
            'y_negative': np.zeros(len(X_negative)) if X_negative is not None else None
        }
    
        return True, train_accuracy
    
    def authenticate(self, features: np.ndarray, threshold: float = 0.5, verbose: bool = False) -> Tuple[bool, float, dict]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        """
        if not self.is_trained:
            return False, 0.0, {}

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ features - —ç—Ç–æ 1D –º–∞—Å—Å–∏–≤
        if features.ndim > 1:
            features = features.flatten()

        if verbose:
            print(f"\n=== –ù–ê–ß–ê–õ–û –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===")
            print(f"–í—Ö–æ–¥—è—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features}")

        # 1. –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ KNN —Å –ü–õ–ê–í–ù–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        features_reshaped = features.reshape(1, -1)
        probabilities = self.model.predict_proba(features_reshaped)[0]

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        knn_probability = 0.5  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 50%
        if len(probabilities) > 1 and 1.0 in self.model.classes_:
            class_1_index = list(self.model.classes_).index(1.0)
            raw_prob = probabilities[class_1_index]
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 10%-90%
            knn_probability = np.clip(raw_prob, 0.1, 0.9)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∫ —Ü–µ–Ω—Ç—Ä—É
            knn_probability = 0.5 + (knn_probability - 0.5) * 0.8

        # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π —Å –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π
        distance_score = 0.5  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        distance_details = {}

        if hasattr(self, 'training_data') and self.training_data is not None:
            from sklearn.metrics.pairwise import euclidean_distances
    
            X_positive = self.training_data
            distances = euclidean_distances(features_reshaped, X_positive)[0]
    
            min_distance = np.min(distances)
            mean_distance = np.mean(distances)
    
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if len(X_positive) > 1:
                train_distances = euclidean_distances(X_positive, X_positive)
                train_distances = train_distances[train_distances > 0]
                mean_train_distance = np.mean(train_distances)
                std_train_distance = np.std(train_distances)
            else:
                mean_train_distance = 1.0
                std_train_distance = 0.5
    
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –ø–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            norm_min = min_distance / (mean_train_distance + 1e-6)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sigmoid-–ø–æ–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤–º–µ—Å—Ç–æ —Ä–µ–∑–∫–æ–≥–æ –æ–±—Ä–µ–∑–∞–Ω–∏—è
            distance_score = 1.0 / (1.0 + np.exp(norm_min - 1.5))  # sigmoid —Å —Ü–µ–Ω—Ç—Ä–æ–º –≤ 1.5
            distance_score = np.clip(distance_score, 0.1, 0.9)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
    
            distance_details = {
                'min_distance': min_distance,
                'mean_distance': mean_distance,
                'mean_train_distance': mean_train_distance,
                'normalized_distance': norm_min
            }

        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º–∏ —à—Ç—Ä–∞—Ñ–∞–º–∏
        feature_score = 0.7  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ö–æ—Ä–æ—à–∞—è –æ—Ü–µ–Ω–∫–∞
        feature_details = {}

        if hasattr(self, 'training_data') and self.training_data is not None:
            X_positive = self.training_data
            train_mean = np.mean(X_positive, axis=0)
            train_std = np.std(X_positive, axis=0)
    
            total_penalty = 0
            feature_penalties = []
            feature_names = ['avg_dwell', 'std_dwell', 'avg_flight', 'std_flight', 'speed', 'total_time']
    
            for i, (feat_val, train_m, train_s, name) in enumerate(zip(features, train_mean, train_std, feature_names)):
                if train_s > 0:
                    z_score_val = abs(feat_val - train_m) / train_s
                    if hasattr(z_score_val, '__len__'):
                        z_score_val = float(z_score_val)
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ —à—Ç—Ä–∞—Ñ—ã
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º sigmoid –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
                    if z_score_val <= 1.0:
                        penalty = 0  # –í –ø—Ä–µ–¥–µ–ª–∞—Ö 1 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è - –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞
                    elif z_score_val <= 2.0:
                        penalty = 0.05 * (z_score_val - 1.0)  # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ 1-2 sigma
                    elif z_score_val <= 3.0:
                        penalty = 0.05 + 0.1 * (z_score_val - 2.0)  # –°—Ä–µ–¥–Ω–∏–π —à—Ç—Ä–∞—Ñ 2-3 sigma
                    else:
                        penalty = 0.15 + 0.15 * min(z_score_val - 3.0, 2.0)  # –ú–∞–∫—Å–∏–º—É–º 30% —à—Ç—Ä–∞—Ñ–∞
                    
                    penalty = min(penalty, 0.3)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ
                else:
                    penalty = 0.0
                    z_score_val = 0.0
        
                feature_penalties.append(penalty)
                feature_details[name] = {
                    'value': float(feat_val),
                    'train_mean': float(train_m),
                    'train_std': float(train_s),
                    'z_score': float(z_score_val),
                    'penalty': penalty
                }
    
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ—ã –±–æ–ª–µ–µ –º—è–≥–∫–æ
            total_penalty = np.mean(feature_penalties)  # –°—Ä–µ–¥–Ω–µ–µ –≤–º–µ—Å—Ç–æ —Å—É–º–º—ã
            feature_score = max(0.2, 1.0 - total_penalty)  # –ú–∏–Ω–∏–º—É–º 20% –≤–º–µ—Å—Ç–æ 10%

        # 4. –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if len(self.training_data) >= 30:
            weights = {'knn': 0.4, 'distance': 0.35, 'features': 0.25}
        elif len(self.training_data) >= 15:
            weights = {'knn': 0.35, 'distance': 0.4, 'features': 0.25}
        else:
            weights = {'knn': 0.3, 'distance': 0.5, 'features': 0.2}

        final_probability = (
            weights['knn'] * knn_probability +
            weights['distance'] * distance_score +
            weights['features'] * feature_score
        )
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        # –ò–∑–±–µ–≥–∞–µ–º –∫—Ä–∞–π–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0% –∏ 100%
        final_probability = np.clip(final_probability, 0.05, 0.95)
        
        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        is_authenticated = final_probability >= threshold

        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        detailed_stats = {
            'knn_confidence': knn_probability,
            'distance_score': distance_score,
            'feature_score': feature_score,
            'final_confidence': final_probability,
            'threshold': threshold,
            'weights': weights,
            'distance_details': distance_details,
            'feature_details': feature_details,
            'training_samples': len(self.training_data) if hasattr(self, 'training_data') else 0
        }

        if verbose:
            print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ï –û–¶–ï–ù–ö–ò:")
            print(f"  KNN: {knn_probability:.3f} (–≤–µ—Å: {weights['knn']})")
            print(f"  Distance: {distance_score:.3f} (–≤–µ—Å: {weights['distance']})")
            print(f"  Features: {feature_score:.3f} (–≤–µ—Å: {weights['features']})")
            print(f"  Final: {final_probability:.3f} (–ø–æ—Ä–æ–≥: {threshold})")
            print(f"  –†–ï–ó–£–õ–¨–¢–ê–¢: {'–ü–†–ò–ù–Ø–¢' if is_authenticated else '–û–¢–ö–õ–û–ù–ï–ù'}")
            print(f"=== –ö–û–ù–ï–¶ –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===\n")

        return is_authenticated, final_probability, detailed_stats
    
    def _generate_balanced_negatives(self, X_positive: np.ndarray, factor: float = 1.5) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–• –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        n_samples = len(X_positive)
        n_features = X_positive.shape[1]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¢–í–û–ò –¥–∞–Ω–Ω—ã–µ
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        std = np.maximum(std, mean * 0.1)

        print(f"\nüîç –°–û–ó–î–ê–ù–ò–ï –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–• –ù–ï–ì–ê–¢–ò–í–û–í:")
        print(f"  –£–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–ª–∞–≤–∏—à: {mean[0]*1000:.1f} ¬± {std[0]*1000:.1f} –º—Å")
        print(f"  –í—Ä–µ–º—è –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏: {mean[2]*1000:.1f} ¬± {std[2]*1000:.1f} –º—Å")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å –ø–µ—á–∞—Ç–∏: {mean[4]:.1f} ¬± {std[4]:.1f} –∫–ª/—Å")

        synthetic_samples = []

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ë–ª–∏–∑–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (40%) - –ù–ï —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ
        close_count = int(n_samples * 0.4)
        print(f"–°–æ–∑–¥–∞–µ–º {close_count} –ë–õ–ò–ó–ö–ò–• –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤...")
        for i in range(close_count):
            sample = mean.copy()
            
            # –ò–∑–º–µ–Ω—è–µ–º 1-2 –ø—Ä–∏–∑–Ω–∞–∫–∞ —É–º–µ—Ä–µ–Ω–Ω–æ
            features_to_change = np.random.choice(6, size=np.random.randint(1, 3), replace=False)
            
            for feat_idx in features_to_change:
                # –£–º–µ—Ä–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: 70%-130% –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ
                factor = np.random.uniform(0.7, 1.3)
                sample[feat_idx] = mean[feat_idx] * factor
            
            # –ù–µ–±–æ–ª—å—à–æ–π —à—É–º
            noise = np.random.normal(0, std * 0.4)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.1)
            synthetic_samples.append(sample)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –î—Ä—É–≥–æ–π —Å—Ç–∏–ª—å (40%)
        different_style_count = int(n_samples * 0.4)
        print(f"–°–æ–∑–¥–∞–µ–º {different_style_count} —Å –î–†–£–ì–ò–ú —Å—Ç–∏–ª–µ–º...")
        for i in range(different_style_count):
            if np.random.random() < 0.5:
                # –ë—ã—Å—Ç—Ä—ã–µ –ø–µ—á–∞—Ç–∞—é—â–∏–µ (–Ω–æ –Ω–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ)
                style_factors = np.array([
                    np.random.uniform(0.6, 0.9),     # –±—ã—Å—Ç—Ä–µ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(0.7, 1.2),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    np.random.uniform(0.5, 0.8),     # –±—ã—Å—Ç—Ä–µ–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(0.6, 1.3),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—É–∑
                    np.random.uniform(1.1, 1.8),     # –≤—ã—à–µ —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(0.6, 0.9)      # –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                ])
            else:
                # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –ø–µ—á–∞—Ç–∞—é—â–∏–µ (–Ω–æ –Ω–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ)
                style_factors = np.array([
                    np.random.uniform(1.1, 1.6),     # –º–µ–¥–ª–µ–Ω–Ω–µ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(0.8, 1.5),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    np.random.uniform(1.2, 2.0),     # –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(1.0, 1.8),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—É–∑
                    np.random.uniform(0.5, 0.9),     # –Ω–∏–∂–µ —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(1.1, 1.7)      # –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                ])
            
            sample = mean * style_factors
            noise = np.random.normal(0, std * 0.3)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.05)
            synthetic_samples.append(sample)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –£–º–µ—Ä–µ–Ω–Ω–æ –¥–∞–ª–µ–∫–∏–µ (20%)
        far_count = n_samples - close_count - different_style_count
        print(f"–°–æ–∑–¥–∞–µ–º {far_count} –£–ú–ï–†–ï–ù–ù–û –¥–∞–ª–µ–∫–∏—Ö...")
        for i in range(far_count):
            # –ë–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω—ã–µ, –Ω–æ –Ω–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è
            factors = np.random.uniform(0.4, 2.5, size=6)
            sample = mean * factors
            noise = np.random.normal(0, std * 0.6)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.02)
            synthetic_samples.append(sample)

        result = np.array(synthetic_samples)
        print(f"  –°–æ–∑–¥–∞–Ω–æ {len(result)} —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
        return result
    
    def save_model(self, user_id: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
    
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        model_data = {
            'model': self.model,
            'n_neighbors': self.n_neighbors,
            'normalization_stats': self.normalization_stats,
            'is_trained': self.is_trained,
            'training_data': self.training_data,
            'test_data': self.test_data
        }
    
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)

    @classmethod
    def load_model(cls, user_id: int) -> Optional['KNNAuthenticator']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        if not os.path.exists(model_path):
            return None
    
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
        
            authenticator = cls(n_neighbors=model_data['n_neighbors'])
            authenticator.model = model_data['model']
            authenticator.normalization_stats = model_data['normalization_stats']
            authenticator.is_trained = model_data['is_trained']
            authenticator.training_data = model_data.get('training_data', None)
            authenticator.test_data = model_data.get('test_data', None)
        
            return authenticator
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)"""
        if not self.is_trained:
            return []
        
        feature_names = [
            'avg_dwell_time',
            'std_dwell_time', 
            'avg_flight_time',
            'std_flight_time',
            'typing_speed',
            'total_typing_time'
        ]
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        X_train = self.model._fit_X
        
        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        variances = np.var(X_train, axis=0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        importance = variances / np.sum(variances)
        
        return list(zip(feature_names, importance))