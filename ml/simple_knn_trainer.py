# ml/simple_knn_trainer.py - –ü—Ä–æ—Å—Ç–∞—è –∏ –Ω–∞–¥–µ–∂–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è

import numpy as np
from typing import Tuple, Dict, List, Optional
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import pickle
import os
from datetime import datetime

from ml.feature_extractor import FeatureExtractor
from config import MODELS_DIR, MIN_TRAINING_SAMPLES

class SimpleKNNTrainer:
    """–ü—Ä–æ—Å—Ç–∞—è –∏ –Ω–∞–¥–µ–∂–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è kNN –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.feature_extractor = FeatureExtractor()
        self.scaler = StandardScaler()
        self.model = None
        self.best_params = {}
        self.training_stats = {}
        
    def prepare_training_data(self, positive_samples: List) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        X_positive = self.feature_extractor.extract_features_from_samples(positive_samples)
        n_positive = len(X_positive)
        
        if n_positive < MIN_TRAINING_SAMPLES:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {n_positive}, –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {MIN_TRAINING_SAMPLES}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        X_negative = self._generate_quality_negatives(X_positive)
        n_negative = len(X_negative)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(n_positive), np.zeros(n_negative)])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_normalized = self.scaler.fit_transform(X)
        
        print(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {n_positive} –≤–∞—à–∏—Ö, {n_negative} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö")
        return X_normalized, y
    
    def _generate_quality_negatives(self, X_positive: np.ndarray) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        n_samples = len(X_positive)
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        std = np.maximum(std, mean * 0.1)
        
        negatives = []
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –ø–µ—á–∞—Ç—å (30%)
        slow_count = int(n_samples * 0.3)
        for _ in range(slow_count):
            sample = mean.copy()
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–∞, —É–º–µ–Ω—å—à–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
            sample[0] *= np.random.uniform(1.5, 2.5)  # avg_dwell_time
            sample[2] *= np.random.uniform(1.8, 3.0)  # avg_flight_time  
            sample[4] *= np.random.uniform(0.4, 0.7)  # typing_speed
            sample[5] *= np.random.uniform(1.5, 2.5)  # total_typing_time
            
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
            noise = np.random.normal(0, std * 0.3)
            sample += noise
            sample = np.maximum(sample, mean * 0.1)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ë—ã—Å—Ç—Ä–∞—è –ø–µ—á–∞—Ç—å (30%)
        fast_count = int(n_samples * 0.3)
        for _ in range(fast_count):
            sample = mean.copy()
            # –£–º–µ–Ω—å—à–∞–µ–º –≤—Ä–µ–º–µ–Ω–∞, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
            sample[0] *= np.random.uniform(0.3, 0.6)  # avg_dwell_time
            sample[2] *= np.random.uniform(0.2, 0.5)  # avg_flight_time
            sample[4] *= np.random.uniform(1.8, 3.5)  # typing_speed
            sample[5] *= np.random.uniform(0.3, 0.7)  # total_typing_time
            
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
            noise = np.random.normal(0, std * 0.3)
            sample += noise
            sample = np.maximum(sample, mean * 0.1)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–µ—á–∞—Ç—å (40%)
        unstable_count = n_samples - slow_count - fast_count
        for _ in range(unstable_count):
            sample = mean.copy()
            # –°–∏–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            sample[1] *= np.random.uniform(3.0, 8.0)  # std_dwell_time
            sample[3] *= np.random.uniform(3.0, 8.0)  # std_flight_time
            
            # –°–ª—É—á–∞–π–Ω–æ –∏–∑–º–µ–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            sample[0] *= np.random.uniform(0.5, 2.0)
            sample[2] *= np.random.uniform(0.5, 2.0)
            sample[4] *= np.random.uniform(0.6, 1.8)
            sample[5] *= np.random.uniform(0.8, 1.5)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
            noise = np.random.normal(0, std * 0.5)
            sample += noise
            sample = np.maximum(sample, mean * 0.05)
            negatives.append(sample)
        
        negatives_array = np.array(negatives)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(X_positive, negatives_array)
        min_dist = np.min(distances)
        mean_dist = np.mean(distances)
        
        print(f"–ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: –º–∏–Ω={min_dist:.2f}, —Å—Ä–µ–¥–Ω–µ–µ={mean_dist:.2f}")
        
        return negatives_array
    
    def train_user_model(self, positive_samples: List) -> Tuple[bool, float, str]:
        """–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            print(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self.prepare_training_data(positive_samples)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.25, random_state=42, stratify=y
            )
            
            # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_params = self._optimize_hyperparameters(X_train, y_train)
            
            # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.model = KNeighborsClassifier(**best_params)
            self.model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            test_accuracy = self._evaluate_model(X_test, y_test)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.training_stats = {
                'user_id': self.user_id,
                'training_samples': len(positive_samples),
                'total_samples': len(X),
                'best_params': best_params,
                'test_accuracy': test_accuracy,
                'training_date': datetime.now().isoformat()
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self._save_model()
            
            print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.2%}")
            return True, test_accuracy, f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {test_accuracy:.2%}"
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False, 0.0, f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}"
    
    def _optimize_hyperparameters(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        param_grid = {
            'n_neighbors': range(3, min(12, len(X_train) // 6)),
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
        
        # Grid Search —Å 5-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        grid_search = GridSearchCV(
            KNeighborsClassifier(),
            param_grid,
            cv=5,
            scoring='accuracy',
            n_jobs=-1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        print(f"CV —Ç–æ—á–Ω–æ—Å—Ç—å: {grid_search.best_score_:.3f}")
        
        self.best_params = grid_search.best_params_
        return grid_search.best_params_
    
    def _evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        print(f"–ú–µ—Ç—Ä–∏–∫–∏:")
        print(f"  Accuracy: {accuracy:.3f}")
        print(f"  Precision: {precision:.3f}")
        print(f"  Recall: {recall:.3f}")
        print(f"  F1-score: {f1:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ROC
        self.training_stats.update({
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'y_test': y_test.tolist(),
            'y_proba': y_proba.tolist()
        })
        
        return accuracy
    
    def predict(self, features: np.ndarray) -> Tuple[bool, float]:
        """–ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –∏ —Å–æ—Å–µ–¥–µ–π
        distances, indices = self.model.kneighbors(features_scaled)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        proba = self.model.predict_proba(features_scaled)[0]
        confidence = proba[1] if len(proba) > 1 else proba[0]
        
        print(f"\nüîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features}")
        print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ: {features_scaled[0]}")
        print(f"   –ú–æ–¥–µ–ª—å k: {self.model.n_neighbors}")
        print(f"   –í–µ—Å–∞: {self.model.weights}")
        print(f"   –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —Å–æ—Å–µ–¥–µ–π: {distances[0]}")
        print(f"   –ò–Ω–¥–µ–∫—Å—ã —Å–æ—Å–µ–¥–µ–π: {indices[0]}")
        print(f"   –°—ã—Ä–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {proba}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
        
        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å –ø–æ—Ä–æ–≥–æ–º 75%
        is_legitimate = confidence >= 0.75
        
        print(f"   –ü–æ—Ä–æ–≥: 0.75")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {'–ü–†–ò–ù–Ø–¢' if is_legitimate else '–û–¢–ö–õ–û–ù–ï–ù'}")
        
        return is_legitimate, confidence
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{self.user_id}_simple_knn.pkl")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    @classmethod
    def load_model(cls, user_id: int) -> Optional['SimpleKNNTrainer']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_simple_knn.pkl")
        
        if not os.path.exists(model_path):
            return None
        
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            trainer = cls(user_id)
            trainer.model = model_data['model']
            trainer.scaler = model_data['scaler']
            trainer.best_params = model_data['best_params']
            trainer.training_stats = model_data.get('training_stats', {})
            
            return trainer
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None
    
    def get_model_info(self) -> Dict:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        return {
            'is_trained': self.model is not None,
            'best_params': self.best_params,
            'training_stats': self.training_stats
        }