# ml/enhanced_model_trainer.py - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è

import numpy as np
from typing import Tuple, Dict, List, Optional
from sklearn.model_selection import (
    train_test_split, cross_val_score, StratifiedKFold, 
    GridSearchCV, validation_curve
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_curve, roc_curve, auc
)
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from datetime import datetime
import os
import pickle
import json

from ml.feature_extractor import FeatureExtractor
from config import MODELS_DIR, DATA_DIR

class EnhancedModelTrainer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.feature_extractor = FeatureExtractor()
        self.scaler = StandardScaler()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.validation_results = {}
        self.best_model = None
        self.best_params = {}
        self.training_history = []
    
    def prepare_training_data(self, positive_samples: List, negative_samples: List = None) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–µ–≥–∞—Ç–∏–≤–æ–≤"""
        
        print(f"\nüî¨ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        X_positive = self.feature_extractor.extract_features_from_samples(positive_samples)
        n_positive = len(X_positive)
        
        print(f"‚úÖ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_positive}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        if negative_samples is None or len(negative_samples) == 0:
            X_negative = self._generate_enhanced_negatives(X_positive)
        else:
            X_negative = self.feature_extractor.extract_features_from_samples(negative_samples)
        
        n_negative = len(X_negative)
        print(f"‚úÖ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_negative}")
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(n_positive), np.zeros(n_negative)])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_normalized = self.scaler.fit_transform(X)
        
        print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"üìà –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {n_positive/(n_positive+n_negative)*100:.1f}% –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö")
        
        return X_normalized, y
    
    def perform_cross_validation(self, X: np.ndarray, y: np.ndarray, cv_folds: int = 5) -> Dict:
        """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        
        print(f"\nüîÑ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø ({cv_folds} fold)")
        
        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤)
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ—Å–µ–¥–µ–π
        k_range = range(1, min(15, len(X)//4))
        cv_results = {}
        
        for k in k_range:
            knn = KNeighborsClassifier(
                n_neighbors=k,
                weights='distance',
                metric='euclidean',
                algorithm='ball_tree'
            )
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            scores = cross_val_score(knn, X, y, cv=cv, scoring='accuracy')
            
            cv_results[k] = {
                'mean_accuracy': float(scores.mean()),  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                'std_accuracy': float(scores.std()),    # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                'scores': scores.tolist()               # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ list
            }
            
            print(f"K={k:2d}: {scores.mean():.3f} ¬± {scores.std():.3f}")
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ K
        best_k = max(cv_results.keys(), key=lambda k: cv_results[k]['mean_accuracy'])
        
        print(f"\nüéØ –õ—É—á—à–µ–µ K: {best_k} (—Ç–æ—á–Ω–æ—Å—Ç—å: {cv_results[best_k]['mean_accuracy']:.3f})")
        
        self.validation_results['cross_validation'] = cv_results
        self.best_params['n_neighbors'] = best_k
        
        return cv_results
    
    def hyperparameter_optimization(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Grid Search"""
        
        print(f"\n‚öôÔ∏è –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        param_grid = {
            'n_neighbors': range(1, min(15, len(X)//3)),
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan', 'minkowski'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree']
        }
        
        # Grid Search —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        knn = KNeighborsClassifier()
        
        grid_search = GridSearchCV(
            knn, param_grid, 
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
            scoring='accuracy',
            n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            verbose=1
        )
        
        grid_search.fit(X, y)
        
        print(f"üèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        print(f"üéØ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {grid_search.best_score_:.3f}")
        
        self.best_params.update(grid_search.best_params_)
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ç–∏–ø—ã
        cv_results_serializable = {}
        for key, value in grid_search.cv_results_.items():
            if isinstance(value, np.ndarray):
                cv_results_serializable[key] = value.tolist()
            elif isinstance(value, (np.int64, np.int32)):
                cv_results_serializable[key] = int(value)
            elif isinstance(value, (np.float64, np.float32)):
                cv_results_serializable[key] = float(value)
            else:
                cv_results_serializable[key] = value
        
        self.validation_results['grid_search'] = {
            'best_params': grid_search.best_params_,
            'best_score': float(grid_search.best_score_),  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
            'cv_results': cv_results_serializable          # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        }
        
        return grid_search.best_params_
    
    def learning_curve_analysis(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        
        print(f"\nüìà –ê–ù–ê–õ–ò–ó –ö–†–ò–í–´–• –û–ë–£–ß–ï–ù–ò–Ø")
        
        from sklearn.model_selection import learning_curve
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        best_knn = KNeighborsClassifier(**self.best_params)
        
        # –†–∞–∑–º–µ—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –≤—ã–±–æ—Ä–æ–∫
        train_sizes = np.linspace(0.1, 1.0, 10)
        
        train_sizes_abs, train_scores, val_scores = learning_curve(
            best_knn, X, y,
            train_sizes=train_sizes,
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
            scoring='accuracy',
            n_jobs=-1
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ —Å–ø–∏—Å–∫–∏
        learning_results = {
            'train_sizes': train_sizes_abs.tolist(),
            'train_scores_mean': train_mean.tolist(),
            'train_scores_std': train_std.tolist(),
            'val_scores_mean': val_mean.tolist(),
            'val_scores_std': val_std.tolist()
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        final_gap = float(train_mean[-1] - val_mean[-1])  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
        if final_gap > 0.1:
            overfitting_status = "–í–´–°–û–ö–ò–ô —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
        elif final_gap > 0.05:
            overfitting_status = "–°–†–ï–î–ù–ò–ô —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
        else:
            overfitting_status = "–ù–ò–ó–ö–ò–ô —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
        
        print(f"üìä –†–∞–∑—Ä—ã–≤ –æ–±—É—á–µ–Ω–∏–µ/–≤–∞–ª–∏–¥–∞—Ü–∏—è: {final_gap:.3f}")
        print(f"üéØ –°—Ç–∞—Ç—É—Å: {overfitting_status}")
        
        learning_results['overfitting_gap'] = final_gap
        learning_results['overfitting_status'] = overfitting_status
        
        self.validation_results['learning_curve'] = learning_results
        
        return learning_results
    
    def detailed_evaluation(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        print(f"\nüéØ –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        final_model = KNeighborsClassifier(**self.best_params)
        final_model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = final_model.predict(X_test)
        y_prob = final_model.predict_proba(X_test)[:, 1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã Python
        evaluation_results = {
            'test_accuracy': float(accuracy_score(y_test, y_pred)),
            'test_precision': float(precision_score(y_test, y_pred)),
            'test_recall': float(recall_score(y_test, y_pred)),
            'test_f1': float(f1_score(y_test, y_pred)),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ list
        }
        
        # ROC-AUC –µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if len(np.unique(y_test)) > 1:
            evaluation_results['roc_auc'] = float(roc_auc_score(y_test, y_prob))
            
            # ROC –∫—Ä–∏–≤–∞—è
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            evaluation_results['roc_curve'] = {
                'fpr': fpr.tolist(),  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ list
                'tpr': tpr.tolist()   # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ list
            }
        
        print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {evaluation_results['test_accuracy']:.3f}")
        print(f"üéØ Precision: {evaluation_results['test_precision']:.3f}")
        print(f"üéØ Recall: {evaluation_results['test_recall']:.3f}")
        print(f"üéØ F1-score: {evaluation_results['test_f1']:.3f}")
        if 'roc_auc' in evaluation_results:
            print(f"üéØ ROC-AUC: {evaluation_results['roc_auc']:.3f}")
        
        self.validation_results['final_evaluation'] = evaluation_results
        self.best_model = final_model
        
        return evaluation_results
    
    def train_with_validation(self, positive_samples: List, negative_samples: List = None) -> Tuple[bool, float, str, Dict]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        
        training_start = datetime.now()
        
        print(f"\nüöÄ –ó–ê–ü–£–°–ö –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
        print(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {training_start.strftime('%H:%M:%S')}")
        print(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ID: {self.user_id}")
        
        try:
            # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self.prepare_training_data(positive_samples, negative_samples)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            if len(X) < 20:
                return False, 0.0, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 20 –æ–±—Ä–∞–∑—Ü–æ–≤)", {}
            
            # 2. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_results = self.perform_cross_validation(X, y)
            
            # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if len(X) >= 30:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                best_params = self.hyperparameter_optimization(X, y)
            else:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Grid Search, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é")
            
            # 4. –ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è
            learning_results = self.learning_curve_analysis(X, y)
            
            # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            evaluation_results = self.detailed_evaluation(X, y)
            
            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            training_end = datetime.now()
            training_duration = (training_end - training_start).total_seconds()
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã
            training_summary = {
                'user_id': int(self.user_id),  # ‚úÖ –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ int
                'training_start': training_start.isoformat(),
                'training_end': training_end.isoformat(),
                'training_duration': float(training_duration),  # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                'dataset_size': int(len(X)),                    # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int
                'n_positive': int(np.sum(y)),                   # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int
                'n_negative': int(len(y) - np.sum(y)),          # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int
                'best_params': self.best_params,
                'validation_results': self.validation_results
            }
            
            self.training_history.append(training_summary)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._save_training_results(training_summary)
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            final_accuracy = evaluation_results['test_accuracy']
            
            print(f"\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_duration:.1f} —Å–µ–∫—É–Ω–¥")
            print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.3f}")
            print(f"üèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.best_params}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            if final_accuracy >= 0.9:
                quality_message = "–û—Ç–ª–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å!"
            elif final_accuracy >= 0.8:
                quality_message = "–•–æ—Ä–æ—à–∞—è –º–æ–¥–µ–ª—å"
            elif final_accuracy >= 0.7:
                quality_message = "–ü—Ä–∏–µ–º–ª–µ–º–∞—è –º–æ–¥–µ–ª—å"
            else:
                quality_message = "–°–ª–∞–±–∞—è –º–æ–¥–µ–ª—å, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö"
            
            success_message = f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ. {quality_message} (—Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.1%})"
            
            return True, final_accuracy, success_message, training_summary
            
        except Exception as e:
            error_message = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}"
            print(f"‚ùå {error_message}")
            import traceback
            traceback.print_exc()
            return False, 0.0, error_message, {}
    
    def _generate_enhanced_negatives(self, X_positive: np.ndarray) -> np.ndarray:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        
        n_samples = len(X_positive)
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        std = np.maximum(std, mean * 0.1)
        
        negatives = []
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–±—Ä–æ—Å—ã (25%)
        outlier_count = n_samples // 4
        for i in range(outlier_count):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—ã–±—Ä–æ—Å—ã –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ 2-4 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
            direction = np.random.choice([-1, 1], size=len(mean))
            magnitude = np.random.uniform(2, 4)
            sample = mean + direction * magnitude * std
            sample = np.maximum(sample, mean * 0.01)  # –ò–∑–±–µ–≥–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (30%)
        opposite_count = int(n_samples * 0.3)
        for i in range(opposite_count):
            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            sample = mean.copy()
            features_to_invert = np.random.choice(len(mean), size=np.random.randint(2, 4), replace=False)
            
            for feat_idx in features_to_invert:
                if feat_idx in [0, 2]:  # –≤—Ä–µ–º–µ–Ω–∞
                    # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ–µ vs –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ–µ
                    factor = np.random.choice([0.2, 4.0])
                elif feat_idx in [1, 3]:  # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                    # –û—á–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ vs –æ—á–µ–Ω—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ
                    factor = np.random.choice([0.1, 3.0])
                elif feat_idx == 4:  # —Å–∫–æ—Ä–æ—Å—Ç—å
                    # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ vs –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ
                    factor = np.random.choice([0.3, 3.0])
                else:  # –æ–±—â–µ–µ –≤—Ä–µ–º—è
                    factor = np.random.choice([0.4, 2.5])
                
                sample[feat_idx] = mean[feat_idx] * factor
            
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –®—É–º–æ–≤—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ (25%)
        noise_count = n_samples // 4
        for i in range(noise_count):
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —à—É–º–∞
            noise_type = np.random.choice(['gaussian', 'uniform', 'exponential'])
            
            if noise_type == 'gaussian':
                noise = np.random.normal(0, std * 2)
            elif noise_type == 'uniform':
                noise = np.random.uniform(-std * 3, std * 3)
            else:  # exponential
                noise = np.random.exponential(std) * np.random.choice([-1, 1], size=len(std))
            
            sample = mean + noise
            sample = np.maximum(sample, mean * 0.05)
            negatives.append(sample)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ú–µ–∂–∫–ª–∞—Å—Å–æ–≤—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (20%)
        boundary_count = n_samples - outlier_count - opposite_count - noise_count
        for i in range(boundary_count):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–∑—Ü—ã –±–ª–∏–∑–∫–æ –∫ –≥—Ä–∞–Ω–∏—Ü–µ —Ä–µ—à–µ–Ω–∏—è
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —à—É–º–∞
            
            # –í—ã–±–∏—Ä–∞–µ–º 2-3 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–∞
            indices = np.random.choice(len(X_positive), size=np.random.randint(2, 4), replace=False)
            weights = np.random.dirichlet(np.ones(len(indices)))  # –°–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞, —Å—É–º–º–∞ = 1
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
            sample = np.zeros_like(mean)
            for idx, weight in zip(indices, weights):
                sample += weight * X_positive[idx]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —à—É–º –¥–ª—è —Å–º–µ—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            directed_noise = np.random.normal(0, std * 0.8)
            sample = sample + directed_noise
            sample = np.maximum(sample, mean * 0.02)
            negatives.append(sample)
        
        result = np.array(negatives)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(result, X_positive)
        min_distances = np.min(distances, axis=1)
        
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ù–ï–ì–ê–¢–ò–í–ù–´–• –ü–†–ò–ú–ï–†–û–í:")
        print(f"  –°–æ–∑–¥–∞–Ω–æ: {len(result)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"  –ú–∏–Ω. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {np.min(min_distances):.3f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.mean(min_distances):.3f}")
        print(f"  –ú–∞–∫—Å. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.max(min_distances):.3f}")
        
        return result
    
    def _save_training_results(self, training_summary: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if self.best_model is not None:
            model_path = os.path.join(MODELS_DIR, f"user_{self.user_id}_enhanced_knn.pkl")
            
            model_data = {
                'model': self.best_model,
                'scaler': self.scaler,
                'best_params': self.best_params,
                'validation_results': self.validation_results,
                'training_summary': training_summary
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π numpy —Ç–∏–ø–æ–≤
        report_path = os.path.join(DATA_DIR, f"training_report_user_{self.user_id}.json")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—è numpy —Ç–∏–ø—ã
            serializable_summary = self._make_json_serializable(training_summary)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_summary, f, indent=2, ensure_ascii=False)
            
            print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –æ—Ç—á–µ—Ç–∞: {e}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            simple_summary = {
                'user_id': self.user_id,
                'training_duration': training_summary.get('training_duration', 0),
                'dataset_size': training_summary.get('dataset_size', 0),
                'final_accuracy': training_summary.get('validation_results', {}).get('final_evaluation', {}).get('test_accuracy', 0),
                'best_params': self.best_params,
                'timestamp': training_summary.get('training_end', datetime.now().isoformat())
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(simple_summary, f, indent=2, ensure_ascii=False)
            
            print(f"üìÑ –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    
    def _make_json_serializable(self, obj):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        if isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (np.complex64, np.complex128)):
            return str(obj)  # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —á–∏—Å–ª–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
        else:
            return obj
    
    @classmethod
    def load_trained_model(cls, user_id: int):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_enhanced_knn.pkl")
        
        if not os.path.exists(model_path):
            return None
        
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            trainer = cls(user_id)
            trainer.best_model = model_data['model']
            trainer.scaler = model_data['scaler']
            trainer.best_params = model_data['best_params']
            trainer.validation_results = model_data['validation_results']
            
            return trainer
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def predict_with_confidence(self, features: np.ndarray) -> Tuple[bool, float, Dict]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        if self.best_model is None:
            return False, 0.0, {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.best_model.predict(features_scaled)[0]
        probabilities = self.best_model.predict_proba(features_scaled)[0]
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = probabilities[1] if len(probabilities) > 1 else probabilities[0]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        distances, indices = self.best_model.kneighbors(features_scaled)
        
        detailed_stats = {
            'prediction': bool(prediction),
            'confidence': float(confidence),
            'probabilities': probabilities.tolist(),
            'nearest_distances': distances[0].tolist(),
            'model_params': self.best_params,
            'validation_accuracy': self.validation_results.get('final_evaluation', {}).get('test_accuracy', 0.0)
        }
        
        return bool(prediction), float(confidence), detailed_stats